{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeestimation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ehrLQsg2l2B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a731e2-6e9d-4a3e-9c5d-6b00e474af9b"
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq -y update\n",
        "!curl https://colab.chainer.org/install | sh -\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg >/dev/null\n",
        "!pip -q install chainerrl\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1379  100  1379    0     0   6074      0 --:--:-- --:--:-- --:--:--  6048\n",
            "********************************************************************************\n",
            "GPU is not enabled!\n",
            "Open \"Runtime\" > \"Change runtime type\" and set \"Hardware accelerator\" to \"GPU\".\n",
            "********************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "THCYi2OpmjDO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/susumuota/gym-modeestimation.git\n",
        "%cd gym-modeestimation\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "\n",
        "# I dunno why but it works...\n",
        "%cd /content/gym-modeestimation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ukyujGSim2kt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_modeestimation\n",
        "\n",
        "def main():\n",
        "    env = gym.make('ModeEstimationEPS00-v0')\n",
        "    obs = env.reset()\n",
        "    for i in range(40):\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        print(env.t, env.n0, action, obs, reward, done, info)\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFa7Nl1Io7A-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "\n",
        "import chainerrl\n",
        "from chainerrl.agents import a2c\n",
        "from chainerrl import experiments\n",
        "from chainerrl import links\n",
        "from chainerrl import misc\n",
        "from chainerrl.optimizers.nonbias_weight_decay import NonbiasWeightDecay\n",
        "from chainerrl import policies\n",
        "from chainerrl import v_function\n",
        "from chainerrl.policy import Policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ShcAPr4uo0sO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class A2CFFSoftmax(chainer.ChainList, a2c.A2CModel):\n",
        "    \"\"\"An example of A2C feedforward softmax policy.\"\"\"\n",
        "\n",
        "    def __init__(self, ndim_obs, n_actions, hidden_sizes=(64, 64)):\n",
        "        self.pi = policies.SoftmaxPolicy(\n",
        "            model=links.MLP(ndim_obs, n_actions, hidden_sizes))\n",
        "        self.v = links.MLP(ndim_obs, 1, hidden_sizes=hidden_sizes)\n",
        "        super().__init__(self.pi, self.v)\n",
        "\n",
        "    def pi_and_v(self, state):\n",
        "        return self.pi(state), self.v(state)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddn_Lj0jpCdK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_reward_filtered(env, reward_filter):\n",
        "    old_step = env.step\n",
        "\n",
        "    def step(action):\n",
        "        observation, reward, done, info = old_step(action)\n",
        "        reward = reward_filter(observation, reward)\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    env.step = step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BVmLBLw4pF22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_env(env_name, process_seed, test=False, reward_scale_factor=1.0):\n",
        "    env = gym.make(env_name)\n",
        "    # Use different random seeds for train and test envs\n",
        "    #process_seed = int(process_seeds[process_idx])\n",
        "    env_seed = 2 ** 32 - 1 - process_seed if test else process_seed\n",
        "    env.seed(env_seed)\n",
        "    # Cast observations to float32 because our model uses float32\n",
        "    env = chainerrl.wrappers.CastObservationToFloat32(env)\n",
        "    # Scale rewards observed by agents\n",
        "    if not test:\n",
        "        misc.env_modifiers.make_reward_filtered(env, lambda x: x * reward_scale_factor)\n",
        "        #make_reward_filtered(env, lambda obs, reward: obs[0])\n",
        "    return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FZWiavtpJ3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_batch_env(num_envs, env_name, seed, test=False, reward_scale_factor=1.0):\n",
        "    process_seeds = np.arange(num_envs) + seed * num_envs\n",
        "    return chainerrl.envs.MultiprocessVectorEnv([(lambda: make_env(env_name, int(process_seeds[idx]), test, reward_scale_factor)) for idx in range(num_envs)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyKSnqoypOa4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
        "gym.logger.set_level(40)\n",
        "\n",
        "#env_name = 'CartPole-v0'\n",
        "#env_name = 'MountainCar-v0'\n",
        "#env_name = 'Pendulum-v0'\n",
        "env_name = 'ModeEstimationEPS00-v0'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUebXwW0pXco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c5bf4edd-dd81-47a7-a3e9-2174beeea420"
      },
      "cell_type": "code",
      "source": [
        "sample_env = make_env(env_name, 0, test=False, reward_scale_factor=1.0)\n",
        "#timestep_limit = sample_env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')\n",
        "timestep_limit = 30\n",
        "obs_space = sample_env.observation_space\n",
        "action_space = sample_env.action_space\n",
        "sample_env.close()\n",
        "\n",
        "print(timestep_limit, obs_space, action_space)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30 Discrete(10) Discrete(11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1YCQ__QUr97i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d06acd6-d90f-4e35-a78e-bc40f436629e"
      },
      "cell_type": "code",
      "source": [
        "arch = 'FFSoftmax'\n",
        "#arch = 'Gaussian'\n",
        "#arch = 'SimpleGaussian'\n",
        "\n",
        "if arch == 'SimpleGaussian':\n",
        "    model = A2CSimpleGaussian(obs_space.low.size, action_space.low.size)\n",
        "if arch == 'Gaussian':\n",
        "    model = A2CGaussian(obs_space.low.size, action_space.low.size)\n",
        "elif arch == 'Beta':\n",
        "    model = A2CBeta(obs_space.low.size, action_space.low.size, action_space.low[0], action_space.high[0])\n",
        "elif arch == 'FFSoftmax':\n",
        "    model = A2CFFSoftmax(obs_space.n, action_space.n)\n",
        "elif arch == 'FFMellowmax':\n",
        "    model = A2CFFMellowmax(obs_space.low.size, action_space.n)\n",
        "\n",
        "model\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.A2CFFSoftmax at 0x7ff9fbaef588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "UK8qEm8nsdNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "861cb798-d33c-44fb-8129-8d4c1fcc6e10"
      },
      "cell_type": "code",
      "source": [
        "lr = 7e-4\n",
        "rmsprop_epsilon = 1e-5\n",
        "alpha = 0.99\n",
        "max_grad_norm = 0.5\n",
        "weight_decay = 0.0\n",
        "\n",
        "optimizer = chainer.optimizers.RMSprop(lr, eps=rmsprop_epsilon, alpha=alpha)\n",
        "optimizer.setup(model)\n",
        "optimizer.add_hook(chainer.optimizer.GradientClipping(max_grad_norm))\n",
        "if weight_decay > 0:\n",
        "    optimizer.add_hook(NonbiasWeightDecay(weight_decay))\n",
        "\n",
        "optimizer\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<chainer.optimizers.rmsprop.RMSprop at 0x7ff9fbadc470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "pbOBw5nzshXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1ecfb7-9894-490e-daab-6bbd758d7fd1"
      },
      "cell_type": "code",
      "source": [
        "num_envs = 8\n",
        "gamma = 0.99\n",
        "#gpu = 0 # -1 for CPU\n",
        "gpu = -1 # -1 for CPU\n",
        "update_steps = 5\n",
        "\n",
        "agent = a2c.A2C(model, optimizer,\n",
        "                    gamma=gamma,\n",
        "                    gpu=gpu,\n",
        "                    num_processes=num_envs,\n",
        "                    update_steps=update_steps)\n",
        "\n",
        "#load = 'exp1/20181210T043953.099247/5000192_finish'\n",
        "load = ''\n",
        "\n",
        "if load:\n",
        "    agent.load(load)\n",
        "\n",
        "\n",
        "agent\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<chainerrl.agents.a2c.A2C at 0x7ff9fb8f9748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "3TGySgVbspMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "reward_scale_factor = 1e-2\n",
        "outdir = 'modeest1'\n",
        "\n",
        "steps         =  1 * 10 ** 3\n",
        "log_interval  =   1 * 10 ** 3\n",
        "eval_interval =  10 * 10 ** 3\n",
        "eval_n_runs = 10\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "outdir = experiments.prepare_output_dir({}, outdir)\n",
        "\n",
        "experiments.train_agent_batch_with_evaluation(\n",
        "    agent=agent,\n",
        "    env=make_batch_env(num_envs, env_name, seed, test=False, reward_scale_factor=reward_scale_factor),\n",
        "    eval_env=make_batch_env(num_envs, env_name, seed, test=True, reward_scale_factor=reward_scale_factor),\n",
        "    steps=steps,\n",
        "    log_interval=log_interval,\n",
        "    eval_n_runs=eval_n_runs,\n",
        "    eval_interval=eval_interval,\n",
        "    outdir=outdir,\n",
        ")\n",
        "\n",
        "end = time.time() - start\n",
        "print(end / 60.0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}